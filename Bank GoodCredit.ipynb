{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa85ec7f",
   "metadata": {},
   "source": [
    "PROJECT TEAM ID: PTID-CDS-JAN-24-1761\n",
    "PROJECT CODE :  Buisness Case for PRCL-0015\n",
    "PROJECT NAME :Bank GoodCredit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc581ed3",
   "metadata": {},
   "source": [
    "# Business Case\n",
    "    Bank GoodCredit wants to predict cred score for current credit card customers. The cred score will denote a customer’s credit worthiness and help the bank in reducing credit default risk.\n",
    "                           Target variable → Bad_label\n",
    "                        -->0 – Customer has Good credit history\n",
    "                        -->1 – Customer has Bad credit history (falls into 30 DPD + bucket)\n",
    "                           Our benchmark model has gini as 37.9.\n",
    "                                               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc87b95",
   "metadata": {},
   "source": [
    "## Domain Analysis\n",
    "                                         \n",
    "Bank credit analysis involves verifying and determining the creditworthiness of a potential client by looking at their financial state, credit reports, and business cash flows.The goal of credit analysis is to determine the level of default risk that a client presents to the company and the losses that the bank will suffer if the client defaults.The risk level that a client presents determines whether the bank will approve or reject the loan application, and if approved, the amount to be awarded.some of the major table this data has and every tables has there own important columns which are Customers Account, Customers Demographics and Customers Enquiry.                                         \n",
    "\n",
    "* Cust_Account :- This table contains customer’s historical accounts data and payments history.\n",
    "* Cust_Demographics :- This table contains customer’s historical enquiry data such as enquiry amount and enquiry purpose.\n",
    "* Cust_Enquiry :- Current customer applications with demographic data\n",
    "* __Note__ that demographics features are renamed as features and obscured in accordance with privacy policies.\n",
    "\n",
    "* I will explain every necesseary columns when i'll do __Data Cleaning__.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5830be89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymysql in c:\\users\\manu8\\anaconda3\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: mysql-connector in c:\\users\\manu8\\anaconda3\\lib\\site-packages (2.2.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install pymysql  \n",
    "!pip install mysql-connector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "63a31f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os               # Handling the Current Working Directory\n",
    "import mysql.connector  #  mysql-connector for making connections to database server\n",
    "import pandas as pd     # import pandas for analyzing, cleaning, exploring, and manipulating data\n",
    "import numpy as np      # import numpy for working with mathematices part/numerical data\n",
    "## visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# for ignoring warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,recall_score,precision_score,classification_report,f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dddec61d",
   "metadata": {},
   "outputs": [
    {
     "ename": "InterfaceError",
     "evalue": "2003: Can't connect to MySQL server on '18.136.157.135:3306' (10065 A socket operation was attempted to an unreachable host)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\mysql\\connector\\network.py:509\u001b[0m, in \u001b[0;36mMySQLTCPSocket.open_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    508\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock\u001b[38;5;241m.\u001b[39msettimeout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection_timeout)\n\u001b[1;32m--> 509\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msockaddr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 10065] A socket operation was attempted to an unreachable host",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInterfaceError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m connection\u001b[38;5;241m=\u001b[39m\u001b[43mmysql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m18.136.157.135\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43muser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdm_team1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDM!$Team&279@20!\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\mysql\\connector\\__init__.py:179\u001b[0m, in \u001b[0;36mconnect\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m CMySQLConnection(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m MySQLConnection(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\mysql\\connector\\connection.py:95\u001b[0m, in \u001b[0;36mMySQLConnection.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool_config_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 95\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnect(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\mysql\\connector\\abstracts.py:716\u001b[0m, in \u001b[0;36mMySQLConnectionAbstract.connect\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    715\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisconnect()\n\u001b[1;32m--> 716\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    717\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_connection()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\mysql\\connector\\connection.py:206\u001b[0m, in \u001b[0;36mMySQLConnection._open_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_protocol \u001b[38;5;241m=\u001b[39m MySQLProtocol()\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_socket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m--> 206\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_handshake()\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_auth(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_user, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_password,\n\u001b[0;32m    209\u001b[0m               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_database, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client_flags, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_charset_id,\n\u001b[0;32m    210\u001b[0m               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ssl)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\mysql\\connector\\network.py:511\u001b[0m, in \u001b[0;36mMySQLTCPSocket.open_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    509\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock\u001b[38;5;241m.\u001b[39mconnect(sockaddr)\n\u001b[0;32m    510\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 511\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mInterfaceError(\n\u001b[0;32m    512\u001b[0m         errno\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2003\u001b[39m, values\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_address(), _strioerror(err)))\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    514\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOperationalError(\u001b[38;5;28mstr\u001b[39m(err))\n",
      "\u001b[1;31mInterfaceError\u001b[0m: 2003: Can't connect to MySQL server on '18.136.157.135:3306' (10065 A socket operation was attempted to an unreachable host)"
     ]
    }
   ],
   "source": [
    "connection=mysql.connector.connect(host=\"18.136.157.135\",\n",
    "                                  user=\"dm_team1\",\n",
    "                                  password=\"DM!$Team&279@20!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c171ad13",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor=connection.cursor()\n",
    "cursor.execute('show databases')\n",
    "for i in cursor:\n",
    "    print(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf0d504",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection=mysql.connector.connect(host=\"18.136.157.135\",\n",
    "                                  user=\"dm_team1\",\n",
    "                                  password=\"DM!$Team&279@20!\",\n",
    "                                  database=\"project_banking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1d38cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting all tables of bank goodcredit\n",
    "db_tables=pd.read_sql_query('show tables',connection)\n",
    "print(db_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a05f315",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql_query('show tables',connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28adf1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'your_query_here' with your actual SQL query\n",
    "# Fetch data from MySQL and store it in a DataFrame\n",
    "Cust_Account=pd.read_sql_query('select* from Cust_Account',connection)\n",
    "Cust_Demographics=pd.read_sql_query('select * from Cust_Demographics',connection)\n",
    "Cust_Enquiry=pd.read_sql_query('select * from Cust_Enquiry',connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d41f57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cust_Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970106e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(Cust_Account,Cust_Demographics,on =\"customer_no\",how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e5e986",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"Bank_GoodCredit\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd72dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fac3c9",
   "metadata": {},
   "source": [
    "# Basic Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3267189",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12f1f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb49df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9460ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a059a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe(include=\"O\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d3e553",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58373a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df5b20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "for i in data.columns:\n",
    "    if (data[i].isnull().sum())/len(data)*100>=40:\n",
    "        df.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89d1450",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f691111e",
   "metadata": {},
   "source": [
    "### Droping columns which has >=40% missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188634ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['closed_dt', 'amt_past_due', 'paymenthistory2', 'creditlimit', 'cashlimit', 'rateofinterest', 'paymentfrequency',\n",
    "           'actualpaymentamount', 'feature_8', 'feature_9', 'feature_10', 'feature_13', 'feature_17', 'feature_18',\n",
    "           'feature_45', 'feature_48', 'feature_49', 'feature_51', 'feature_53', 'feature_57', 'feature_61', 'feature_73',\n",
    "           'feature_74'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0e2a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e6df7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d45a599",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ffbb74",
   "metadata": {},
   "source": [
    "### Now we are going to drop unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2cbae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop([\"reporting_dt\",\"paymenthistory1\",\"dt_opened_y\",\"entry_time\",\"feature_2\",\"feature_20\",\"feature_21\",\n",
    "          \"feature_22\",\"feature_24\",\"feature_46\",\"feature_47\",\"feature_54\",\"feature_75\",\"feature_77\",\"last_paymt_dt\",\"paymt_str_dt\",\"paymt_end_dt\",\"feature_5\",\"feature_6\",\"feature_62\",\"feature_70\",\n",
    "           \"feature_79\",\"feature_39\",\"dt_opened_x\",\"upload_dt\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abe1805",
   "metadata": {},
   "source": [
    "### Now, i'm going to drop that columns which has no variety in values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df35627f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop([\"owner_indic\",\"feature_11\",\"feature_19\",\"feature_23\",\"feature_25\",\"feature_31\",\"feature_33\",\"feature_42\",\n",
    "          \"feature_55\",\"feature_58\",\"feature_59\",\"feature_60\",\"feature_67\",\"feature_76\",\"feature_78\",\"feature_52\",],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b7ec38",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option('display.max_rows',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a90cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()  # checking null value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6fdc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install missingno\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3789cc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "## this plot is used to detect the null values\n",
    "msno.matrix(data,color=(0.60,0.300,0.300),fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1713d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data.columns:\n",
    "    print(f\"======= { i } ========\\n\")\n",
    "    print(data[i].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95deaca6",
   "metadata": {},
   "source": [
    "## Filling all null value by the help of Measure of Central Tendency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212fb451",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data[\"opened_dt\"].isnull(),\"opened_dt\"] = \"13-Apr-12\"\n",
    "data.loc[data[\"high_credit_amt\"].isnull(),\"high_credit_amt\"] = np.mean(data.high_credit_amt)\n",
    "data.loc[data[\"feature_1\"].isnull(),\"feature_1\"] = \"Platinum Maxima\"\n",
    "data.loc[data[\"feature_3\"].isnull(),\"feature_3\"] = 682.0\n",
    "data.loc[data[\"feature_4\"].isnull(),\"feature_4\"] = 3.000000\n",
    "data.loc[data[\"feature_7\"].isnull(),\"feature_7\"] = np.mean(data.feature_7)\n",
    "data.loc[data[\"feature_12\"].isnull(),\"feature_12\"] = \"PM1\"\n",
    "data.loc[data[\"feature_14\"].isnull(),\"feature_14\"] = 12.000000\n",
    "data.loc[data[\"feature_15\"].isnull(),\"feature_15\"] = \"SA03\"\n",
    "data.loc[data[\"feature_16\"].isnull(),\"feature_16\"] = \"AS03\"\n",
    "data.loc[data[\"feature_26\"].isnull(),\"feature_26\"] = 0.000000\n",
    "data.loc[data[\"feature_27\"].isnull(),\"feature_27\"] = \"Graduate\"\n",
    "data.loc[data[\"feature_28\"].isnull(),\"feature_28\"] = \"New Delhi\"\n",
    "data.loc[data[\"feature_29\"].isnull(),\"feature_29\"] = np.mean(data.feature_29)\n",
    "data.loc[data[\"feature_30\"].isnull(),\"feature_30\"] = 2010.0\n",
    "data.loc[data[\"feature_32\"].isnull(),\"feature_32\"] = \"Self\"\n",
    "data.loc[data[\"feature_34\"].isnull(),\"feature_34\"] = 2.0\n",
    "data.loc[data[\"feature_35\"].isnull(),\"feature_35\"] = 42759.59392727169\n",
    "data.loc[data[\"feature_36\"].isnull(),\"feature_36\"] = \"Private Ltd. Co.\"\n",
    "data.loc[data[\"feature_37\"].isnull(),\"feature_37\"] = \"Banking/Financial Services\"\n",
    "data.loc[data[\"feature_38\"].isnull(),\"feature_38\"] = \"MANAGER\"\n",
    "data.loc[data[\"feature_40\"].isnull(),\"feature_40\"] = 0.0\n",
    "data.loc[data[\"feature_41\"].isnull(),\"feature_41\"] = 11.0\n",
    "data.loc[data[\"feature_43\"].isnull(),\"feature_43\"] = \"New Delhi\"\n",
    "data.loc[data[\"feature_44\"].isnull(),\"feature_44\"] = np.mean(data.feature_44)\n",
    "data.loc[data[\"feature_50\"].isnull(),\"feature_50\"] = \"Y\"\n",
    "data.loc[data[\"feature_63\"].isnull(),\"feature_63\"] = \"2010-0\"\n",
    "data.loc[data[\"feature_64\"].isnull(),\"feature_64\"] = 10.0\n",
    "data.loc[data[\"feature_65\"].isnull(),\"feature_65\"] = 157.0\n",
    "data.loc[data[\"feature_66\"].isnull(),\"feature_66\"] = np.mean(data.feature_66)\n",
    "data.loc[data[\"feature_68\"].isnull(),\"feature_68\"] = 1.0\n",
    "data.loc[data[\"feature_69\"].isnull(),\"feature_69\"] = np.mean(data.feature_69)\n",
    "data.loc[data[\"feature_71\"].isnull(),\"feature_71\"] = 10.0\n",
    "data.loc[data[\"feature_72\"].isnull(),\"feature_72\"] = \"R\"\n",
    "data.loc[data[\"feature_56\"].isnull(),\"feature_56\"] = 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0552bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()  # checking null value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a72585c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno ## this plot is used to detect the null values\n",
    "msno.matrix(data,color=(0.60,0.300,0.300),fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acd4a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_into_datetime(columns):\n",
    "    data[columns]=pd.to_datetime(data[columns])\n",
    "    \n",
    "# Converting the columns into Datetime using for loop\n",
    "for i in ['opened_dt']:  \n",
    "    convert_into_datetime(i) \n",
    "    \n",
    "# Create the new columns with month \n",
    "data['opened_dt_year']=data['opened_dt'].dt.year\n",
    "\n",
    "data.drop('opened_dt',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91047873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here i'm going to split the feature_63 columns for getting only year\n",
    "\n",
    "data.feature_63 = data['feature_63'].str.split('-',expand=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81ccce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd957134",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change columns type according to their value some columns having object type but it should in int type \n",
    "\n",
    "data.high_credit_amt = data.high_credit_amt.astype(int)\n",
    "data.feature_3 = data.feature_3.astype(int)\n",
    "data.feature_4 = data.feature_4.astype(int)\n",
    "data.feature_14 = data.feature_14.astype(int)\n",
    "data.feature_26 = data.feature_26.astype(int)\n",
    "data.feature_30 = data.feature_30.astype(int)\n",
    "data.feature_34 = data.feature_34.astype(int)\n",
    "data.feature_40 = data.feature_40.astype(int)\n",
    "data.feature_44 = data.feature_41.astype(int)\n",
    "data.feature_56 = data.feature_56.astype(int)\n",
    "data.feature_63 = data.feature_63.astype(int)\n",
    "data.feature_64 = data.feature_64.astype(int)\n",
    "data.feature_65 = data.feature_65.astype(int)\n",
    "data.feature_68 = data.feature_68.astype(int)\n",
    "data.feature_71 = data.feature_71.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9da933d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8958f47d",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e3bdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "catagorical=data.describe(include=\"O\")\n",
    "catagorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a399f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric=data.describe(exclude=\"O\")\n",
    "numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536ce7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='feature_1', hue='Bad_label', data=data)\n",
    "plt.title(\"Credit Cards vs Bad Labels\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e0e4e5",
   "metadata": {},
   "source": [
    "### Insights\n",
    "\n",
    "* Most of the person having Platinum Maxima and Platinum Deligh Cards.\n",
    "* person who has Golf Card they having 50-50 chance of good or bad Credit Score (bad or good labels).\n",
    "* person who have RBL bank Fun+,Insignia and Platinum Cricke Cards they have perfect credit score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20c7f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data.feature_3)# asign histplot for feature_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25aa63fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='feature_27',hue = 'Bad_label', data=data)#assign countplot between feature_27 and Bad Labels\n",
    "plt.title(\"customers education vs Bad Labels\")# assign title name of distributions\n",
    "plt.xticks(rotation=90)#increasing font of x-axis and rorating 90 degree \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6b7095",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='feature_28',hue = 'Bad_label',data=data)#assign countplot between feature_28 and Bad Labels\n",
    "plt.title(\"Customer State vs Bad Labels\") # assign title name of distributions\n",
    "plt.xticks(fontsize = 7,rotation = 90)#increasing font of x-axis and rorating 90 degree \n",
    "plt.yticks(fontsize = 11)#increasing font of y-axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21c3921",
   "metadata": {},
   "source": [
    "### Insights\n",
    "\n",
    "* Most of the Customers belong from New Delhi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dceeee50",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='feature_32',hue ='Bad_label',data=data)#assign countplot between feature_32 and Bad Labels\n",
    "plt.title(\"Customers stay vs Bad Labels\") # assign title name of distributions\n",
    "plt.xticks(rotation=90)#increasing font of x-axis and rorating 90 degree \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1440bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='feature_36',hue = 'Bad_label',data=data) #assign countplot between feature_36 and Bad Labels\n",
    "plt.title(\"customers Business vs Bad Labels\") # assign title name of distributions\n",
    "plt.xticks(rotation=90)#increasing font of x-axis and rorating 90 degree \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ad18d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='feature_37',hue = 'Bad_label',data=data) #assign countplot between feature_37 and Bad Labels\n",
    "plt.title(\"customers Ocupations vs Bad Labels\") # assign title name of distributions\n",
    "plt.xticks(rotation=90)#increasing font of x-axis and rorating 90 degree \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb848d0",
   "metadata": {},
   "source": [
    "## Final Conclusions of Insights :-\n",
    "\n",
    "\n",
    "* Most of the person having Platinum Maxima and Platinum Deligh Cards.\n",
    "* Person who has Golf Card they having 50-50 chance of good or bad Credit Score (bad or good labels).\n",
    "* Person who have RBL bank Fun+,Insignia and Platinum Cricke Cards they have perfect credit score.\n",
    "* Most of the customers have good credit score.\n",
    "* Few chance of that customers who has completed Post Graduate, Graduate and MBA/MMS their Credit score are Bad.\n",
    "* Most of the Customers belong from New Delhi.\n",
    "* Most of the Customer having Credit Cards who belong from Private Ltd.Co.\n",
    "* Customers who started their bussiness in Partership they maintain their credit score.\n",
    "* Most chance of that Customers who doing job in Banking/Financial services they taking credit cards.\n",
    "* Most of the customers who has crdit cards they stay at their on home."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed49a848",
   "metadata": {},
   "source": [
    "# Checking Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78337aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting that columns which type are integers\n",
    "data1= data.select_dtypes(include='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822667d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e513e985",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(100,50))\n",
    "plotnumber=1\n",
    "for i in data1:\n",
    "    plt.subplot(5,4,plotnumber)\n",
    "    sns.boxplot(x=data[i])\n",
    "    plotnumber=plotnumber+1\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.xlabel(i,fontsize=20)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a129f85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5f9d59",
   "metadata": {},
   "source": [
    "### Insights\n",
    "*  By using boxplot we find out the outliers Columns\n",
    "*  Columns are \n",
    "               1)acct_type\n",
    "               2)high_credit_amt\n",
    "               3)cur_balance_amt\n",
    "               4)feature_65\n",
    "* The outliers are filling the median values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7a2f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_boundaries(data,variables):\n",
    "    Q1=data[variables].quantile(0.25)\n",
    "    Q3=data[variables].quantile(0.75)\n",
    "    IQR=Q3-Q1\n",
    "    lowerboundary=Q1-1.5*IQR\n",
    "    upperboundary=Q3+1.5*IQR\n",
    "    return lowerboundary,upperboundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0613c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_acct_type,upper_acct_type=find_boundaries(data,'acct_type')\n",
    "print('upper bondary for acct_type',upper_acct_type)\n",
    "print('lower boundary for acct_type',lower_acct_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3565e8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['acct_type']>16.0,'acct_type']=np.median(data.acct_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1d65e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_high_credit_amt,upper_high_credit_amt=find_boundaries(data,'high_credit_amt')\n",
    "print('upper bondary for high_credit_amt',upper_high_credit_amt)\n",
    "print('lower boundary for high_credit_amt',lower_high_credit_amt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7be15e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['high_credit_amt']>301662.5,'high_credit_amt']=np.median(data.high_credit_amt)\n",
    "data.loc[data['high_credit_amt']<-142597.5,'high_credit_amt']=np.median(data.high_credit_amt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8522d948",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_cur_balance_amt,upper_cur_balance_amt=find_boundaries(data,'cur_balance_amt')\n",
    "print('upper bondary for cur_balance_amt',upper_cur_balance_amt)\n",
    "print('lower boundary for cur_balance_amt',lower_cur_balance_amt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f251cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['cur_balance_amt']>52666.25,'cur_balance_amt']=np.median(data.cur_balance_amt)\n",
    "data.loc[data['cur_balance_amt']<-31599.75,'cur_balance_amt']=np.median(data.cur_balance_amt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fcc512",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_feature_3,upper_feature_3=find_boundaries(data,'feature_3')\n",
    "print('upper bondary for feature_3',upper_feature_3)\n",
    "print('lower boundary for feature_3',lower_feature_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06afdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['feature_3']>825.0,'feature_3']=np.median(data.feature_3)\n",
    "data.loc[data['feature_3']<609.0,'feature_3']=np.median(data.feature_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccfaea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_feature_65,upper_feature_65=find_boundaries(data,'feature_65')\n",
    "print('upper bondary for feature_65',upper_feature_65)\n",
    "print('lower boundary for feature_65',lower_feature_65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84361460",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['feature_65']>370.0,'feature_65']=np.median(data.feature_65)\n",
    "data.loc[data['feature_65']<-198.0,'feature_65']=np.median(data.feature_65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280e7400",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(100,50))\n",
    "plotnumber=1\n",
    "for i in data1:\n",
    "    plt.subplot(5,4,plotnumber)\n",
    "    sns.boxplot(x=data[i])\n",
    "    plotnumber=plotnumber+1\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.xlabel(i,fontsize=20)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d2823b",
   "metadata": {},
   "source": [
    "## Insights \n",
    "* Finally we remove the outliers\n",
    "* Remaning data point are not outliers ,They are near the maximum values,minimum value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64c164a",
   "metadata": {},
   "source": [
    "## Conversion of categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca39d245",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le=LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6fb8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding(data):\n",
    "    le=LabelEncoder()\n",
    "    for i in data.select_dtypes('object'):\n",
    "        data[i]=le.fit_transform(data[i])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f918254",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=encoding(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbd3baa",
   "metadata": {},
   "source": [
    "# Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84303f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler=MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc9a0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['customer_no','high_credit_amt','cur_balance_amt','feature_3','feature_7','feature_15','feature_16','feature_29','feature_30','feature_35','feature_38','feature_63','feature_66','feature_69','opened_dt_year']]=scaler.fit_transform(data[['customer_no','high_credit_amt','cur_balance_amt','feature_3','feature_7','feature_15','feature_16','feature_29','feature_30','feature_35','feature_38','feature_63','feature_66','feature_69','opened_dt_year']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2663e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca63036",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc56fb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.corr()  # checking corelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55252eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(28,28))#increase plot size\n",
    "sns.heatmap(data.corr(),annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5811779",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop([\"feature_1\",\"feature_63\",\"feature_68\",\"feature_69\",\"feature_41\"],axis=1,inplace=True) # droping columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45d2388",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(28,28))#increase plot size\n",
    "sns.heatmap(data.corr(),annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f33992",
   "metadata": {},
   "source": [
    "# Insights\n",
    "* Finally we droped higly correlated columns,here there is no correlation then moving to next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a9778d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now selecting independent variable and dependent variables\n",
    "x = data.drop(\"Bad_label\",axis=1)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2ec7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.Bad_label\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6987dbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Bad_label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e82a390",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bbb73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  # import train test split for spliting the data\n",
    "# creating x_train,x_test,y_train,y_test\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25,random_state=43) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18012fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3aee46",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8bc1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392b6093",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ea315d",
   "metadata": {},
   "source": [
    "# Data Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d453b17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE  # assign SMOTE for handling unbalanced data\n",
    "sm = SMOTE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb9630b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sm,y_sm = sm.fit_resample(x_train,y_train)  # balancing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbba978",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter  # checking data balanced or not\n",
    "print(Counter(y_train))\n",
    "print(Counter(y_sm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3278cf7f",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca88ca3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8765f5ad",
   "metadata": {},
   "source": [
    "# Logistic Regression Model\n",
    "*Logistic Regression is a statistical technique used for binary classification, where the goal is to predict whether an instance belongs to one of two classes (usually represented as 0 or 1). Despite its name, logistic regression is used for classification tasks, not regression tasks. It models the probability that an instance belongs to the positive class, making it a probabilistic algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4164f0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model1=LogisticRegression()\n",
    "model1.fit(x_sm,y_sm)\n",
    "y_pred1=model1.predict(x_test)\n",
    "y_pred_lr_train = model1.predict(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b54f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm=confusion_matrix(y_test,y_pred1)\n",
    "recall=recall_score(y_test,y_pred1,average='weighted')\n",
    "precision=precision_score(y_test,y_pred1,average='weighted')\n",
    "f1score=f1_score(y_test,y_pred1,average='weighted')\n",
    "accuracy=accuracy_score(y_test,y_pred1)\n",
    "print(\"CM:\",cm)\n",
    "print(\"Recall:\",recall)\n",
    "print(\"Precision:\",precision)\n",
    "print(\"F1 Score:\",f1score)\n",
    "print(\"Accuracy:\",accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95bf73f",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier\n",
    "\n",
    "* A Decision Tree is a popular and intuitive machine learning algorithm used for both classification and regression tasks. It models data as a tree-like structure of decisions and their possible consequences. Each internal node of the tree represents a decision based on a feature, and each leaf node represents a prediction or a class label.\n",
    "\n",
    "**Here's how a Decision Tree works:**\n",
    "\n",
    "**Selecting the Best Feature:**\n",
    "\n",
    "* The algorithm starts by selecting the feature that best splits the data into different classes or reduces the variance of the target variable. The selection criterion is typically based on measures like Gini impurity, entropy, or mean squared error, depending on whether the task is classification or regression.\n",
    "\n",
    "**Splitting the Data:**\n",
    "\n",
    "* The selected feature is used to split the data into subsets. Each subset corresponds to a unique value of the selected feature.\n",
    "\n",
    "**Repeating the Process:**\n",
    "\n",
    "* The above steps are recursively applied to the subsets created at each internal node. The algorithm keeps selecting features that provide the best splits, and the tree continues to grow until a stopping criterion is met.\n",
    "\n",
    "**Stopping Criteria:**\n",
    "\n",
    "* The tree growth can be stopped based on various criteria, such as:\n",
    "\n",
    "Maximum depth: The maximum number of levels the tree can have.\n",
    "Minimum samples per leaf: The minimum number of samples required to create a leaf node.\n",
    "Minimum samples per split: The minimum number of samples required to perform a split.\n",
    "Leaf Node Assignments:\n",
    "Once the tree is built, each leaf node is assigned a class label (in the case of classification) or a predicted value (in the case of regression). The majority class or the mean/median value of the target variable in the leaf node's subset is typically used.\n",
    "\n",
    "**Prediction:*\n",
    "* To make a prediction for a new instance, the instance is traversed down the tree from the root to a leaf node. The predicted class label or value of the leaf node is then used as the final prediction.\n",
    "\n",
    "**Advantages of Decision Trees:**\n",
    "\n",
    "* Intuitive: Decision Trees are easy to understand and visualize, making them useful for explaining decisions to non-experts.\n",
    "* Non-parametric: They can capture complex relationships without assuming a specific functional form.\n",
    "* Feature Importance: Decision Trees can provide insights into feature importance and feature interactions.\n",
    "\n",
    "**Limitations of Decision Trees:**\n",
    "\n",
    "* Prone to Overfitting: Without proper pruning or limiting growth, Decision Trees can become overly complex and overfit the training data.\n",
    "**Instability:**\n",
    "* Small changes in the data can lead to different tree structures, making them sensitive to variations.\n",
    "\n",
    "**Bias towards Dominant Classes:**\n",
    "\n",
    "* In classification, they can favor dominant classes if the dataset is imbalanced.\n",
    "* To address some of the limitations, ensemble techniques like Random Forest and Gradient Boosting are often used, which combine multiple Decision Trees to improve performance and robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924dfb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model2=DecisionTreeClassifier()\n",
    "model2.fit(x_sm,y_sm)\n",
    "y_pred2=model2.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4c1e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,accuracy_score,recall_score,precision_score,classification_report,f1_score\n",
    "cm=confusion_matrix(y_test,y_pred2)\n",
    "recall=recall_score(y_test,y_pred2,average='weighted')\n",
    "precision=precision_score(y_test,y_pred2,average='weighted')\n",
    "f1score=f1_score(y_test,y_pred2,average='weighted')\n",
    "accuracy=accuracy_score(y_test,y_pred2)\n",
    "print(\"CM:\",cm)\n",
    "print(\"Recall:\",recall)\n",
    "print(\"Precision:\",precision)\n",
    "print(\"F1 Score:\",f1score)\n",
    "print(\"Accuracy:\",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02b4b89",
   "metadata": {},
   "source": [
    "# Random Forest Classifier\n",
    "* Random Forest is a powerful ensemble learning technique that combines multiple individual decision trees to create a more accurate and robust predictive model. It's commonly used for classification and regression tasks, offering improved performance over single decision trees by mitigating their limitations.\n",
    "\n",
    "**Here's how Random Forest works:**\n",
    "\n",
    "**Bootstrap Aggregating (Bagging):**\n",
    "\n",
    "* Random Forest employs a technique called bagging, where multiple subsets of the training data are created through random sampling with replacement. Each subset is used to train a separate decision tree. This introduces diversity in the training data, reducing the likelihood of overfitting.\n",
    "\n",
    "\n",
    "**Random Feature Selection:**\n",
    "\n",
    "* In addition to using random subsets of data, Random Forest introduces randomness in feature selection during the construction of each individual tree. At each split, the algorithm considers only a subset of features, rather than all features. This helps to decorrelate the trees and improve generalization.\n",
    "\n",
    "**Building Multiple Trees:**\n",
    "\n",
    "* The Random Forest algorithm builds a specified number of decision trees using the bootstrapped subsets of data and the random feature subsets. Each decision tree is built using the standard Decision Tree algorithm (e.g., using Gini impurity or entropy for classification, or mean squared error for regression).\n",
    "\n",
    "**Voting or Averaging:**\n",
    "\n",
    "* For classification tasks, the final prediction of the Random Forest is determined through a majority vote among the individual decision trees. For regression tasks, predictions are averaged across the trees.\n",
    "\n",
    "**Advantages of Random Forest:**\n",
    "\n",
    "**Reduced Overfitting:**\n",
    "* By aggregating multiple trees and introducing randomness, Random Forest reduces overfitting compared to individual decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5456272d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model3=RandomForestClassifier()\n",
    "model3.fit(x_sm,y_sm)\n",
    "y_pred3=model3.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556beea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,accuracy_score,recall_score,precision_score,classification_report,f1_score\n",
    "cm=confusion_matrix(y_test,y_pred3)\n",
    "recall=recall_score(y_test,y_pred3,average='weighted')\n",
    "precision=precision_score(y_test,y_pred3,average='weighted')\n",
    "f1score=f1_score(y_test,y_pred3,average='weighted')\n",
    "accuracy=accuracy_score(y_test,y_pred3)\n",
    "print(\"CM:\",cm)\n",
    "print(\"Recall:\",recall)\n",
    "print(\"Precision:\",precision)\n",
    "print(\"F1 Score:\",f1score)\n",
    "print(\"Accuracy:\",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0bfe91",
   "metadata": {},
   "source": [
    "# XGBRFClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae747edd",
   "metadata": {},
   "source": [
    "* XGBRFClassifier is a classifier based on the XGBoost algorithm that is specifically designed to work with random forests. XGBoost (Extreme Gradient Boosting) is a popular and powerful machine learning algorithm known for its performance and versatility in both classification and regression tasks. While XGBoost typically utilizes boosted trees, the XGBRFClassifier is a variation that focuses on the random forest ensemble structure. It is part of the XGBoost library and offers benefits similar to traditional random forest classifiers but with some enhancements from the XGBoost framework.\n",
    "\n",
    "**Here's a breakdown of the key features and aspects of the XGBRFClassifier:**\n",
    "\n",
    "**Ensemble Structure:**\n",
    "\n",
    "* The XGBRFClassifier constructs an ensemble of decision trees, similar to traditional random forest classifiers. However, it leverages the techniques and optimizations of the XGBoost framework.\n",
    "\n",
    "**Gradient Boosting Principles:**\n",
    "\n",
    "* While the XGBRFClassifier uses random forest-style decision trees, it still benefits from some of the boosting principles of XGBoost. This includes gradient boosting, which iteratively improves the model by focusing on correcting the mistakes of previously added trees.\n",
    "\n",
    "**Tree Building Process:**\n",
    "\n",
    "* The classifier constructs decision trees using a random subset of features and a subset of the training data (bootstrap aggregation or bagging). This helps to introduce diversity and reduce overfitting.\n",
    "\n",
    "**Regularization and Pruning:**\n",
    "\n",
    "* Like XGBoost, the XGBRFClassifier applies regularization techniques to prevent overfitting. It also supports early stopping to avoid constructing too many trees that may not improve the model's performance.\n",
    "\n",
    "**Hyperparameter Tuning:**\n",
    "\n",
    "* The XGBRFClassifier comes with a variety of hyperparameters that can be tuned to optimize its performance for specific tasks. These hyperparameters control aspects like the depth of trees, learning rate, subsampling ratio, and more.\n",
    "\n",
    "**Parallel and Distributed Computing:**\n",
    "\n",
    "* The XGBoost library is designed to efficiently use parallel and distributed computing resources. This can lead to faster training times and improved scalability.\n",
    "\n",
    "**Feature Importance and Visualization:**\n",
    "\n",
    "* XGBRFClassifier provides tools for assessing feature importance, enabling you to understand which features contribute the most to the model's predictions.\n",
    "\n",
    "* The XGBRFClassifier is particularly useful when you want to harness the benefits of both random forest-style ensembles and the optimizations and regularization techniques provided by XGBoost. It can be applied to a wide range of classification tasks, including those with complex relationships and high-dimensional data.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac70151",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRFClassifier\n",
    "model4=XGBRFClassifier() ##n_estimators = 500, seed = 123\n",
    "model4.fit(x_sm,y_sm)\n",
    "y_pred4=model4.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5837f491",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm=confusion_matrix(y_test,y_pred4)\n",
    "recall=recall_score(y_test,y_pred4,average='weighted')\n",
    "precision=precision_score(y_test,y_pred4,average='weighted')\n",
    "f1score=f1_score(y_test,y_pred4,average='weighted')\n",
    "accuracy=accuracy_score(y_test,y_pred4)\n",
    "print(\"CM:\",cm)\n",
    "print(\"Recall:\",recall)\n",
    "print(\"Precision:\",precision)\n",
    "print(\"F1 Score:\",f1score)\n",
    "print(\"Accuracy:\",accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a75416b",
   "metadata": {},
   "source": [
    "# GradientBoostingClassifier\n",
    "\n",
    "\n",
    "* The GradientBoostingClassifier is a powerful and popular machine learning algorithm used for classification tasks. It is part of the gradient boosting family of algorithms, which are designed to improve the predictive accuracy of models by combining the predictions of multiple weak learners (typically decision trees) in an additive manner. The GradientBoostingClassifier is implemented in scikit-learn and is based on the gradient boosting framework.\n",
    "\n",
    "**Here's an explanation of the key concepts and features of the GradientBoostingClassifier:**\n",
    "\n",
    "**Gradient Boosting Framework:**\n",
    "\n",
    "* Gradient boosting is an ensemble learning technique that builds an additive model in a forward stage-wise manner. Each stage constructs a new weak learner that corrects the errors made by the previous stage.\n",
    "\n",
    "**Weak Learners (Decision Trees):**\n",
    "\n",
    "* The base learners in the GradientBoostingClassifier are often shallow decision trees, also known as weak learners. These trees have limited depth and can be thought of as simple rules applied to the data.\n",
    "\n",
    "**Boosting Iterations:**\n",
    "\n",
    "* The algorithm performs a series of boosting iterations. At each iteration, a new decision tree is added to the ensemble to correct the mistakes made by the existing ensemble.\n",
    "\n",
    "**Residuals and Gradient Descent:**\n",
    "\n",
    "* The new tree is fitted to the negative gradient of the loss function with respect to the current predictions. This guides the tree construction to reduce the errors of the previous predictions.\n",
    "\n",
    "**Learning Rate:**\n",
    "\n",
    "* A learning rate parameter controls the contribution of each new tree to the ensemble. A lower learning rate can prevent overfitting by shrinking the impact of each new tree.\n",
    "\n",
    "**Regularization:**\n",
    "\n",
    "* The GradientBoostingClassifier employs regularization techniques to prevent overfitting, such as controlling the depth of the trees and using a minimum number of samples per leaf.\n",
    "\n",
    "**Subsampling:**\n",
    "\n",
    "* Random subsampling of the data can be applied to each boosting iteration to introduce randomness and improve generalization.\n",
    "\n",
    "**Feature Importance:**\n",
    "\n",
    "* The algorithm provides a measure of feature importance based on how often features are used in decision trees and how much they contribute to reducing the loss.\n",
    "\n",
    "**Early Stopping:**\n",
    "\n",
    "* To avoid overfitting, the algorithm can be stopped early based on the performance on a validation set. This prevents adding too many trees that may lead to overfitting.\n",
    "\n",
    "* The GradientBoostingClassifier is effective for a wide range of classification tasks, including both binary and multi-class problems. It can capture complex relationships in data and often achieves competitive performance compared to other advanced algorithms. However, it may require tuning of hyperparameters to achieve optimal performance and to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01618d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "model5=GradientBoostingClassifier()\n",
    "model5.fit(x_sm,y_sm)\n",
    "y_pred5=model5.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc529508",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm=confusion_matrix(y_test,y_pred5)\n",
    "recall=recall_score(y_test,y_pred5,average='weighted')\n",
    "precision=precision_score(y_test,y_pred5,average='weighted')\n",
    "f1score=f1_score(y_test,y_pred5,average='weighted')\n",
    "accuracy=accuracy_score(y_test,y_pred5)\n",
    "print(\"CM:\",cm)\n",
    "print(\"Recall:\",recall)\n",
    "print(\"Precision:\",precision)\n",
    "print(\"F1 Score:\",f1score)\n",
    "print(\"Accuracy:\",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d28abf8",
   "metadata": {},
   "source": [
    "# MLPClassifier- ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dcb8de",
   "metadata": {},
   "source": [
    "##### * MLPClassifier is a type of ANN (Artificial Neural Network). It is a multilayer perceptron (MLP) classifier, which is a feedforward neural network that can be used for classification tasks.\n",
    "\n",
    "* An MLP consists of multiple layers of neurons, each layer is fully connected to the following one. The neurons in the input layer receive the input data, and the neurons in the output layer produce the output predictions. The neurons in the hidden layers perform the intermediate computations.\n",
    "\n",
    "* The MLPClassifier class in scikit-learn implements a multi-layer perceptron (MLP) algorithm that trains using Backpropagation. It can be used for both binary and multi-class classification tasks.\n",
    "\n",
    "* The MLPClassifier class has the following parameters:\n",
    "\n",
    "              hidden_layer_sizes: A list of the number of neurons in each hidden layer.\n",
    "              activation: The activation function for the hidden layers.\n",
    "              solver: The algorithm used to train the model.\n",
    "              alpha: The learning rate.\n",
    "              max_iter: The maximum number of iterations.\n",
    "              random_state: The random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248bb530",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "model6= MLPClassifier( hidden_layer_sizes=(100,50,2), activation='relu',solver='adam',\n",
    "                       learning_rate_init=0.0001,\n",
    "                       max_iter=150,random_state=78) ## model object creation max_iter=Stopping parameter\n",
    "model6.fit(x_sm,y_sm) ## training the data\n",
    "y_pred6=model6.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c564ccda",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm=confusion_matrix(y_test,y_pred6)\n",
    "recall=recall_score(y_test,y_pred6,average='weighted')\n",
    "precision=precision_score(y_test,y_pred6,average='weighted')\n",
    "f1score=f1_score(y_test,y_pred6,average='weighted')\n",
    "accuracy=accuracy_score(y_test,y_pred6)\n",
    "print(\"CM:\",cm)\n",
    "print(\"Recall:\",recall)\n",
    "print(\"Precision:\",precision)\n",
    "print(\"F1 Score:\",f1score)\n",
    "print(\"Accuracy:\",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35155ce",
   "metadata": {},
   "source": [
    "## Cross Validation in All Algorithms\n",
    "* Cross-validation is a fundamental technique in machine learning used to evaluate the performance of algorithms and models, as well as to tune hyperparameters and assess their generalization ability. It involves splitting the dataset into multiple subsets to simulate different training and testing scenarios, helping to ensure that the model's performance is representative and reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98085bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_models=[model1,model2,model3,model4,model5,model6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdc369e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "accuracy_cvs=[]\n",
    "for i in list_of_models:\n",
    "    cvs=cross_val_score(i,x,y,cv=5,scoring='accuracy')\n",
    "    p=cvs.mean()\n",
    "    accuracy_cvs.append(p)\n",
    "accuracy_cvs   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9408972",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_of_algorithms=['LogisticRegression','DecisionTreeClassifier','RandomForestClassifier','XGBRFClassifier','GradientBoostingClassifier','MLPClassifier']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcb7bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "sns.barplot(x=names_of_algorithms,y=accuracy_cvs)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9d1c89",
   "metadata": {},
   "source": [
    "# **Finalizing the algorithm**\n",
    "\n",
    "* Among all algorithms \"GradientBoostingClassifier\"aves the more accuracy.So finalized algorithm is radientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c3f94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "pkl.dump(model3,open(\"mobile.pkl\",\"wb\"))\n",
    "read=pkl.load(open(\"mobile.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d00359",
   "metadata": {},
   "outputs": [],
   "source": [
    "independent_input_data=[0.258297,10,0.374619,0.366569,0.409302,3,0.083813,10,12,0.600551,0.570058,2,8,39,0.481703,0.647059,4,2,0.030000,6,1,0.464324,0,43,0,1,21,21,15,0.379517,21,1,0.888889]\n",
    "e=read.predict([independent_input_data])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ebd8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95eca9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[166664]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a55d37",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "In conclusion, Bank GoodCredit appears to have a promising project on its hands. The bank's strategy is aligned with industry trends and market demands, and it has taken steps to mitigate potential risks and challenges. The project's focus on innovation and technology positions it well for future growth and competitiveness in the banking sector.\n",
    "\n",
    "Bank GoodCredit's commitment to market analysis, financial projections, and compliance with regulatory requirements showcases a comprehensive approach to project planning. Additionally, its emphasis on marketing and customer acquisition indicates a strong intent to build a robust customer base.\n",
    "\n",
    "However, the ultimate success of the project will depend on its execution, the effectiveness of the team, and the dynamic nature of the banking industry. Continuous monitoring and adaptation to changing market conditions will be crucial for achieving the projected financial goals and maintaining a positive reputation among customers and stakeholders.\n",
    "\n",
    "It's important to note that this conclusion is based on a general hypothetical scenario. For a more accurate and detailed conclusion, specific project details and data would be required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad478cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b074510d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dd2106",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540ef9ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42afd214",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0499ce95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
